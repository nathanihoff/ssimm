<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Lecture 12: Models of ordinal data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jeffrey B. Lewis" />
    <meta name="date" content="2019-05-22" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 12: Models of ordinal data
### Jeffrey B. Lewis
### May 22, 2019

---




&lt;style&gt; 
.big-left {
  width: 60%;
  height: 92%;
  float: left;
}

.small-right {
  width: 40%;
  height: 92%;
  float: right;
}

.big-right {
  width: 60%;
  height: 92%;
  float: right;
}

.small-left {
  width: 40%;
  height: 92%;
  float: left;
}

&lt;/style&gt;

## Things to do today

* Put ordinal logit/probit into practice 

* .blue[Example] from the text: modeling voters' perceptions of 
President Obama's ideology.


---
class: center, middle, inverse

# A simple application of ordinal logit

---
## 2016 NES pilot


```r
nes &lt;- read_csv("ch8data/anes_pilot_2016.csv") %&gt;%
       dplyr::select(caseid, weight, lcbo, birthyr, 
              follow, faminc, pid7, educ, race) %&gt;%
       mutate( white = ifelse(race==1, 1, 0),
               age = 2017-birthyr,
               pid7 = ifelse(pid7&gt;7, NA, pid7),
               lcbo = ifelse(lcbo&gt;7, NA, lcbo),
               faminc =ifelse(faminc&gt;16, NA, faminc)) 
head(nes)
```

```
## # A tibble: 6 x 11
##   caseid weight  lcbo birthyr follow faminc  pid7  educ  race white   age
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1      1  0.951     5    1960      1      4     1     5     1     1    57
## 2      2  2.67      5    1957      2      8     4     6     1     1    60
## 3      3  1.43      1    1963      1      1     6     5     1     1    54
## 4      4  0.914     4    1980      1     12     1     5     1     1    37
## 5      5  0.264     1    1974      1     10     5     2     1     1    43
## 6      6  1.46      1    1958      1      7     4     6     1     1    59
```

---
## "How liberal is President Obama?"

.pull-left[

```r
gg &lt;- ggplot(nes, aes(x=lcbo)) + 
    geom_histogram(aes(y = ..count../sum(..count..)), fill="grey40") +
    theme_minimal() + 
    scale_x_continuous(breaks=1:7,
      labels=c("Extremely liberal",
                "Liberal",
                "Slightly liberal",
                "Moderate",
                "Slightly conservative",
                "Conservative",
                "Extremely Conservative")) +
    theme(axis.text.x = 
            element_text(angle = 45, 
                         hjust = 1)) +
    ylab("Frequency") +
    xlab("How liberal is President Obama?") +
    theme(
      axis.text = element_text(size=16),
      axis.title = element_text(size=16)
    )
```
]
.pull-right[
![](oprobit_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]


---
## Ordered logit in R: The `MASS::polr` 

.pull-left[

```r
ores &lt;- MASS::polr(as.ordered(lcbo) ~ follow + pid7 + age + educ + faminc + white, 
             data=nes, Hess=TRUE)
summary(ores)$coefficients
```

```
##              Value  Std. Error     t value
## follow  0.62137438 0.075183977   8.2647181
## pid7   -0.38411818 0.031517259 -12.1875502
## age    -0.00733335 0.003694276  -1.9850572
## educ   -0.02926644 0.043350215  -0.6751164
## faminc -0.04083085 0.020800109  -1.9630112
## white  -0.11418273 0.137891174  -0.8280641
## 1|2    -1.58920200 0.337191045  -4.7130611
## 2|3    -0.52572279 0.333423278  -1.5767429
## 3|4     0.33248228 0.335523635   0.9909355
## 4|5     1.94109397 0.351864335   5.5165977
## 5|6     2.71992128 0.372861504   7.2947227
## 6|7     3.57473560 0.419757780   8.5161866
```
.red[
Small differences due to extra subsampling done in the textbook. 
]
]
.pull-right[
&lt;img src="img/Fig8-1.png" width="60%" /&gt;
]

---
## "How liberal is President Obama?" by respondent party id

Make dataset with rows for each value of `pid7` and all other
variable held at their means:


```r
nes.means &lt;- nes %&gt;% 
              summarize_all(mean, na.rm=T)
plot.dat &lt;- bind_rows(rep(list(nes.means),7)) %&gt;%
              mutate(pid7 = 1:7)
plot.dat
```

```
## # A tibble: 7 x 11
##   caseid weight  lcbo birthyr follow faminc  pid7  educ  race white   age
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1   600.     1.  2.39   1968.   1.73   5.61     1  3.22  1.59 0.729  49.1
## 2   600.     1.  2.39   1968.   1.73   5.61     2  3.22  1.59 0.729  49.1
## 3   600.     1.  2.39   1968.   1.73   5.61     3  3.22  1.59 0.729  49.1
## 4   600.     1.  2.39   1968.   1.73   5.61     4  3.22  1.59 0.729  49.1
## 5   600.     1.  2.39   1968.   1.73   5.61     5  3.22  1.59 0.729  49.1
## 6   600.     1.  2.39   1968.   1.73   5.61     6  3.22  1.59 0.729  49.1
## 7   600.     1.  2.39   1968.   1.73   5.61     7  3.22  1.59 0.729  49.1
```

---
## "How liberal is President Obama?" by respondent party id


```r
pprobs &lt;- as.data.frame(predict(ores, type="probs", newdata=plot.dat)) %&gt;%
            mutate(pid7=plot.dat$pid7) 
pprobs 
```

```
##           1         2          3          4           5           6
## 1 0.1802363 0.2088230 0.21129389 0.28206673 0.059951201 0.032276137
## 2 0.2440450 0.2391738 0.20483902 0.22874403 0.043215004 0.022575850
## 3 0.3215821 0.2570032 0.18549362 0.17771442 0.030624445 0.015660861
## 4 0.4103805 0.2580481 0.15782305 0.13335671 0.021439952 0.010801579
## 5 0.5054312 0.2420465 0.12724860 0.09740597 0.014882224 0.007420452
## 6 0.6000909 0.2128636 0.09817693 0.06971905 0.010268980 0.005083747
## 7 0.6878232 0.1767075 0.07317975 0.04916765 0.007056665 0.003476335
##             7 pid7
## 1 0.025352700    1
## 2 0.017407239    2
## 3 0.011921404    3
## 4 0.008150073    4
## 5 0.005565081    5
## 6 0.003796843    6
## 7 0.002588979    7
```

```r
pprobs &lt;- pprobs %&gt;%
            gather(oblc, phat, -pid7) 
head( pprobs )
```

```
##   pid7 oblc      phat
## 1    1    1 0.1802363
## 2    2    1 0.2440450
## 3    3    1 0.3215821
## 4    4    1 0.4103805
## 5    5    1 0.5054312
## 6    6    1 0.6000909
```

---
## "How liberal is President Obama?" by respondent party id

.pull-left[

```r
gg &lt;- ggplot(pprobs, aes(x=pid7, y=phat, col=oblc)) + 
    geom_line(size=1.5) + 
    geom_point(size=2) +  
    theme_minimal() +
    scale_x_continuous(breaks=1:7) +
    theme(
      axis.text = element_text(size=16),
      axis.title = element_text(size=16)
    )
```
]
.pull-right[
![](oprobit_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---
## Adding confidence intervals 


```r
library(mvtnorm)
```

```
## Warning: package 'mvtnorm' was built under R version 3.5.2
```

```r
sims &lt;- 250
sim_ores &lt;- list(sims)
pseudo_ores &lt;- ores
k &lt;- length(coef(ores))
for (i in 1:sims) {
  theta &lt;- as.numeric(rmvnorm(1, mean=c(coef(ores), ores$zeta), sigma=vcov(ores)))
  pseudo_ores$coefficients &lt;- theta[1:k]
  pseudo_ores$zeta &lt;- theta[(k+1):length(theta)]
  sim_ores[[i]] &lt;- as.data.frame(cbind(i,1:7, predict(pseudo_ores, 
                                   newdata=plot.dat, type="probs")))
  names(sim_ores[[i]]) &lt;- c("iter","pid7",1:7) 
}
sim_ores &lt;- bind_rows(sim_ores) %&gt;%
              gather(oblc, phat, -iter, -pid7) %&gt;%
              group_by(pid7, oblc) %&gt;%
              dplyr::summarize(phat.se = sd(phat),
                               phat.mean = mean(phat),
                               phat.lo = phat.mean - 1.96*phat.se,
                               phat.hi = phat.mean + 1.97*phat.se)
```


---
## Adding confidence intervals 
.pull-left[

```r
gg &lt;- gg + 
        geom_ribbon(data=sim_ores,
            aes(ymin=phat.lo,
                ymax=phat.hi, y=NULL,
                fill=oblc, color=NULL), 
            alpha=0.2)
```
]
.pull-right[
![](oprobit_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]


---
class:center, inverse, middle

# Probing the ordinal logit model's core assumption

---
## Assumption: Ordinal data come from coarsening of a latent continuous scale

In ordinal logit, 

$$ y_i = \begin{cases}
            0 &amp; \text{if `\(y^*_i &lt; \tau_1\)`} \cr
            1 &amp; \text{if `\(\tau_1 \le y^*_i &lt; \tau_2\)`} \cr
            2 &amp; \text{if `\(\tau_2 \le y^*_i &lt; \tau_3\)`} \cr
            \vdots &amp; \vdots \cr
            K &amp; \text{if $ y^*_i \ge \tau_K$} \cr
          \end{cases} $$
          
where `\(y^*_i\)` is the `\(i\)`th individual's location of the latent continuous scale, the observable `\(y_i \in \{0,1,\dots,K\}\)` indicates which interval of a partition along `\(y^*\)` that `\(y^*_i\)` falls, and the cutpoints `\(\tau_k\)` for `\(k \in \{0,1,\dots,K\}\)` define the limitis of those intervals.


---
## Assumption: Ordinal data come from coarsening a latent continuous scale

Ordinal logit assumes that 

`$$y^*_i ~ \sim ~ \mathrm{Logistic(\mu_i,1)}$$`
and 

`$$\mu_i = \beta_1 x_{i1} + \beta_1 x_{i1} + \dots + \beta_J x_{iJ}.$$`
As we have seen, identification of the latent `\(y^*\)` is created by omitting an intercept from the equation above (setting `\(\beta_0 = 0\)`) and setting the scale parameter of the logistic distribution to 1.  

If this description of `\(y^*\)` is correct then would be able to consistently estimate `\(\beta\)`s regardless of how many intervals `\(y^*\)` is broken into ( `\(K+1\)` ) or where `\(\tau\)`s are located as long as we have at least one `\(\tau\)` ( `\(K&gt;0\)` ) 

The more intervals that we have the less coarse the measurement of the underlying `\(y^*\)` provided by `\(y\)` is. 

The more coarse the measurement, the less efficient the estimates.

---
## The effect of coarseness

To see the loss of efficiency from coarsening, let's generate simulated data with `\(K=50\)` and then collapse the number of intervals in the resulting data until `\(K=1\)` (two intervals) remain.  For each, level of coarseness (value of `\(K\)`), we will fit the ordered logit model and note the estimated `\(\beta_1\)` and the associated standard error. 

.pull-left[

```r
N &lt;- 5000
x &lt;- rnorm(N)
xb &lt;- 1*x 
ystar &lt;- rlogis(N, location=xb)
# Create 50 taus
tau &lt;- c(-Inf, sort( runif(50, 
                           min(ystar), 
                           max(ystar)) ), Inf)
y &lt;-  as.integer(cut(ystar, tau)) - 1
```
]
.pull-right[
![](oprobit_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]


---
## The effect of coarseness


```r
result &lt;- list()
for (K in 35:1) {
   yK &lt;- as.integer(cut(y, breaks=K+1)) - 1
   if (K==1) {
     resK &lt;- glm(as.factor(yK) ~ x, family=binomial(logit))
     result[[K]] &lt;- tibble(K=K, 
                         b1=resK$coefficients[2],
                         b1.se = sqrt( vcov(resK)[2,2]))
   }
   else {
     resK &lt;- MASS::polr(as.factor(yK) ~ x, Hess=TRUE)
     result[[K]] &lt;- tibble(K=K, 
                         b1=resK$coefficients,
                         b1.se = sqrt( vcov(resK)[1,1]))
     
   }
}
result &lt;- bind_rows(result)
```

---
## Estimated `\(\beta_1\)` as a function of coarseness

![](oprobit_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;


---
## Ordinal logit as a cumulative probability model

Note that we could also coarsen the outcome variable `\(y\)` into `\(K\)` different binary variables each of which indicates whether `\(y_i\)` is greater than some value `\(k \in \{0, 1, \dots, K\}\)`.

It follows from the ordinal logit model that

`$$\textrm{Logit}( \textrm{Prob}(y_i \ge k) ) = -\tau_k + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_J x_{iJ}.$$`
Thus, if we graph the log odds that `\(y_i \ge k\)` as a function of say `\(x_1\)` for a each value of `\(k\)` (holding the other `\(x\)`s fixed), the relatioship between the log-odds and `\(x_1\)` will be linear for every `\(k\)` and those lines will be parallel across values of `\(k\)`.

A more general model might say that 

`$$\textrm{Logit}( \textrm{Prob}(y_i \ge k) ) = -\tau_k + \beta_{1k} x_{i1} + \beta_{2k} x_{i2} + \dots + \beta_{Jk} x_{iJ}.$$`
Here the the log odds are still a linear function of `\(x_1\)` for each value of `\(k\)`, but the lines are not parallel across values of `\(k\)` (because `\(\beta_1\)` varies by `\(k\)`).

The parameters of this more general model can be estimated by a series of simple binary logits.  If the ordinal logit model is correct, then the `\(\beta\)`s will not vary by `\(k\)`.


---
## Binary logit estimates versus ordered logit estimates


```r
logit_res &lt;- matrix(NA, 6,15)
b_pred_values &lt;- list() 
for (i in 1:6) {
  this_res &lt;-  glm( I(lcbo&gt;i) ~  follow + pid7 + age + educ + faminc + white,
                    family=binomial(logit), data=nes )
  b_pred_values[[i]] &lt;- tibble(cut=i,
                           pid = plot.dat$pid7,
                           xb = predict(this_res, newdata = plot.dat,
                                          type="link"))
  logit_res[i,] &lt;-  c(i, coef(this_res), diag(vcov(this_res))) 
}
b_pred_values &lt;- bind_rows(b_pred_values)
logit_res &lt;- as.data.frame(logit_res)
vnames &lt;- c("inter", "follow", "pid7", "age", 
            "educ", "faminc", "white")
names(logit_res) &lt;- c("split", paste0("est.", vnames), paste0("se.", vnames))

logit_res &lt;- logit_res %&gt;%
  gather(coef,value,-split) %&gt;%
  separate(coef, into=c("type","coef")) %&gt;%
  spread(type, value) %&gt;%
  mutate( ci_lo = est - 1.96*se,
          ci_hi = est + 1.96*se)
```


---
## Binary logit estimates versus ordinal logit estimates


```r
ores_dat &lt;- data.frame(summary(ores)$coefficients[1:6,]) %&gt;%
              mutate(coef = rownames(data.frame(ores$coefficients)))
gg &lt;- ggplot(logit_res %&gt;% filter(coef != "inter"), 
                            aes(x=split, y=est, ymin=ci_lo, ymax=ci_hi)) +
       geom_ribbon(fill='grey60', alpha=0.4) +
       geom_line() + geom_point(size=2.5) +
       facet_wrap(~coef, scales="free_y") +
       scale_x_continuous(breaks=1:6,
                          labels=c("1|2", "2|3", "3|4", "4|5", "5|6", "6|7")) +
       geom_hline(data=ores_dat, 
                  aes(yintercept=Value),
                  col='blue')  +
       geom_hline(data=ores_dat, aes(yintercept=Value + 1.96*Std..Error),
                  linetype='dotted', col='blue', size=1.5)  +
       geom_hline(data=ores_dat, aes(yintercept=Value - 1.96*Std..Error),
                  linetype='dotted', col='blue', size=1.5) +
       theme_minimal() +
       theme(
         axis.text = element_text(size=16),
         axis.title = element_text(size=16),
         strip.text.x = element_text(size=16)
       ) 
```

---
class:center
##  Binary logit estimates versus ordered logit estimates

![](oprobit_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---
## Ordinal logit's "parallel lines" assumption

![](oprobit_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;


---
## Ordinal logit's proportional-odds assumption

![](oprobit_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;


---
## Ordinal logit versus ordinal logit - probabilities 

![](oprobit_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;



---
## Takeaways

* Ordinal logit and probit are constructed as latent variables models in which it is assumed that there is a continuous variable whose values are partitioned into the observable ordered categories.

* Usually, the meaning of the underlying latent scale is arbitrary is and is determined by setting `\(\beta_0\)` or `\(\tau_1=0\)` and setting `\(\sigma=1\)`.

* In those cases, interpretation is generally made in terms of the probabilities of the observable categories.

* Ordinal logit and probit models can be tested by further coarsening the ordinal outcome measure into a series of binary measures.  If the ordinal logit/probit is correct, the coefficients of each of the binary logit/probit models should be similar to those recovered in the ordinal logit/probit.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
